{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e47cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5430bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '처리', '가', '너무', '재밌', '어서', '밥', '먹', '는', '것', '도', '가끔', '까먹', '어요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "print(mecab.morphs('자연어처리가너무재밌어서밥먹는것도가끔까먹어요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9eb8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is john . \n",
      "Python version\n",
      "3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]\n",
      "Version info.\n",
      "sys.version_info(major=3, minor=12, micro=11, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "def pad_punctuation(sentence, punc):\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is john.\"\n",
    "\n",
    "print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))\n",
    "\n",
    "\n",
    "import sys  \n",
    "print(\"Python version\") \n",
    "print(sys.version) \n",
    "print(\"Version info.\") \n",
    "print(sys.version_info)\n",
    "\n",
    "\n",
    "\n",
    "#문장부호 처리 => 양옆에 띄어쓰기, 즉 공백 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191487a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n",
      "FIRST, OPEN THE FIRST CHAPTER.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.lower())\n",
    "\n",
    "# First & first는 같은의미인데도 다른걸로 취급될수 있기에 소문자로 변환\n",
    "\n",
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "# Q. sentence의 모든 단어를 대문자로 바꿔보세요.\n",
    "# 힌트: upper() 함수를 사용해 보세요!\n",
    "# [[YOUR CODE]]\n",
    "print(sentence.upper())\n",
    "#대문자로도 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf7786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"He is a ten-year-old boy.\"\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n",
    "#첫번째는 대괄호 안의 문자들을 제외한 모든 문자, 두번째는 공백으로 대체, 세번째는 대상 문자열 (즉 sentence 변수))\n",
    "print(sentence)\n",
    "\n",
    "#ten-year-old와 seven-year-old, 그 외의 수많은 나이 표현들을 각각의 단어 취급을 해버리는 일은 원하지않아요\n",
    "#그래서 특수문자는 제거합니다\n",
    "#고로. 사용할 알파벳과 기호들을 정의해 이를 제외하곤 모두 제거\n",
    "\n",
    "# a-z (all lowercase letters)\n",
    "\n",
    "# A-Z (all uppercase letters)\n",
    "\n",
    "# . (the period)\n",
    "\n",
    "# , (the comma)\n",
    "\n",
    "# ? (the question mark)\n",
    "\n",
    "# ! (the exclamation mark)\n",
    "\n",
    "#^ here means 'not'. \n",
    "\n",
    "# [a-zA-Z.,?!] would mean: \"Match any single character that IS a letter, a period, a comma, a question mark, or an exclamation mark.\n",
    " \n",
    "#  [^a-zA-Z.,?!] means the exact opposite: \"Match any single character that IS NOT a letter, a period, a comma, a question mark, or an exclamation mark.\"\n",
    " \n",
    "# re.sub is the function for \"substitute\" in Python's regular expression (re) library.\n",
    "\n",
    "# Its job is to find a pattern in a string and replace it with something else.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d72a6",
   "metadata": {},
   "source": [
    "re.sub is the function for \"substitute\" in Python's regular expression (re) library.\n",
    "\n",
    "Its job is to find a pattern in a string and replace it with something else.\n",
    "\n",
    "How It Works\n",
    "The basic structure is: re.sub(pattern, repl, string)\n",
    "\n",
    "pattern: The regex pattern you want to find.\n",
    "\n",
    "In your example: ([^a-zA-Z.,?!])\n",
    "\n",
    "This translates to: \"Find any character that is NOT a letter or one of those punctuation marks.\"\n",
    "\n",
    "repl: The replacement string you want to insert.\n",
    "\n",
    "In your example: \" \"\n",
    "\n",
    "This translates to: \"Replace what you found with a single space.\"\n",
    "\n",
    "string: The variable or text you want to search in.\n",
    "\n",
    "In your example: sentence\n",
    "\n",
    "This was: \"He is a ten-year-old boy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f60caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk . \n",
      "but my teacher had been with me several weeks before i understood that everything has a name . \n",
      "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered . \n",
      "some one was drawing water and my teacher placed my hand under the spout . \n",
      "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly . \n",
      "i stood still ,  my whole attention fixed upon the motions of her fingers . \n",
      "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me . \n",
      "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand . \n",
      "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free ! \n",
      "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From The Project Gutenberg\n",
    "# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n",
    "\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk.\n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered.\n",
    "Some one was drawing water and my teacher placed my hand under the spout.\n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly.\n",
    "I stood still, my whole attention fixed upon the motions of her fingers.\n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me.\n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand.\n",
    "That living word awakened my soul, gave it light, hope, joy, set it free!\n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\"\n",
    "\n",
    "def cleaning_text(text, punc, regex):\n",
    "    # 노이즈 유형 (1) 문장부호 공백추가\n",
    "    for p in punc:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n",
    "    text = re.sub(regex, \" \", text).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "#공백기반 토큰화\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .\n",
    "but my teacher had been with me several weeks before i understood that everything has a name .\n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .\n",
    "some one was drawing water and my teacher placed my hand under the spout .\n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .\n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .\n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .\n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .\n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !\n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away .\n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split()\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)\n",
    "#이 방법은 days & day가 구분되어 따로 저장이 되지만 이거 하나가지고 뭘 할수도 없는거라 어쩔수 없다고 취급한다고 함ㄴ (더 나은 방법이 있다고함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #형태소 기반 토큰화 - (명사) 뜻을 가진 가장 작은 말의 단위.\n",
    "\n",
    "# # KoNLPy는 내부적으로 5가지의 형태소 분석 Class를 포함하고 있습니다 천하무적인 것은 (아직은) 없으니, 각 분석기를 직접 테스트해보고 적합한 것을 선택해 사용하면 됩니다.\n",
    "\n",
    "# mecab 이 압도적으로 빠름\n",
    "# 띄어쓰기가 없는 문장: 그래도 꼬꼬마, KOMORAN, mecab 분석기는 생각보다 훌륭한 분석 품질을 보여주며 선전했네요.\n",
    "# 자소 분리 및 오탈자가 포함된 문장: 코모란 만이 자소로 보고 학습 함\n",
    "# 긴 문장: 대부분 무난하게 함\n",
    "\n",
    "# 사용할 데이터의 특성(띄어쓰기 유무 등)이나 개발 환경(Python, Java)에 따라서 적합한 형태소 분석기를 고려해야함\n",
    "# 연산 속도가 중요하다면 mecab을 최우선으로 고려해야하며, 심지어 분석 품질도 상위권으로 보여짐\n",
    "# 자소 분리나 오탈자에 대해서도 어느 정도 분석 품질이 보장되야 한다면 KOMORAN 사용을 고려\n",
    "# 한나눔과 khaiii는 일부 케이스에 대한 분석 품질, 꼬꼬마는 분석 시간에서 약간 아쉬운 점이 보임\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae5b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb01aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['테스트', '문장', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "print(mecab.morphs(\"테스트 문장입니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d6a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hannanum] \n",
      "[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n",
      "[Kkma] \n",
      "[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
      "[Komoran] \n",
      "[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "[Mecab] \n",
      "[('코로나', 'NNP'), ('바이러스', 'NNG'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNBC'), ('12', 'SN'), ('월', 'NNBC'), ('중국', 'NNP'), ('우한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('한', 'XSV+ETM'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKB'), ('확산', 'NNG'), ('된', 'XSV+ETM'), (',', 'SC'), ('새로운', 'VA+ETM'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n",
      "[Okt] \n",
      "[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤\n",
    "# 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.\n",
    "\n",
    "# →\n",
    "\n",
    "# <unk>는 2019년 12월 중국 <unk>에서 처음 발생한 뒤\n",
    "# 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.\n",
    "\n",
    "# 만약 위 문장을 영문으로 번역해야 한다면 어떨까요? 핵심인 단어 코로나바이러스와 우한을 모른다면 제대로 해낼 수 있을 리가 없습니다. \n",
    "# 이를 OOV(Out-Of-Vocabulary) 문제라고 합니다. 이처럼 새로 등장한(본 적 없는) 단어에 대해 약한 모습을 보일 수밖에 없는 기법들이기에, \n",
    "# 이를 해결하고자 하는 시도들이 있었습니다. 그리고 그것이 우리가 다음 스텝에서 배울, Wordpiece Model이죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ca695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. OOV(Out-Of-Vocabulary) 문제가 뭐였고 왜 BPE 문제를 해결 했을까요?\n",
    "# => 간단히 요약하면, **OOV(Out-Of-Vocabulary)**는 모델의 '단어장'에 없는 단어가 등장했을 때, 이를 처리하지 못하고 <UNK> (Unknown) 같은 토큰으로 뭉개버리는 문제입니다.\n",
    "# **BPE(Byte Pair Encoding)**는 이 문제를 모든 단어를 '부분단어(subword)'로 쪼개는 방식을 사용해 해결했습니다. \n",
    "# 아무리 새로운 단어가 등장해도, BPE는 그 단어를 이미 학습한 부분단어들의 조합(최악의 경우 글자 단위)으로 표현할 수 있으므로 <UNK> 토큰이 필요 없게 됩니다.\n",
    "\n",
    "# bpe 예시:\n",
    "\n",
    "# aaabdaaabac # 가장 많이 등장한 바이트 쌍 \"aa\"를 \"Z\"로 치환합니다.\n",
    "# →\n",
    "# ZabdZabac   # \"aa\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "# Z=aa        # 그다음 많이 등장한 바이트 쌍 \"ab\"를 \"Y\"로 치환합니다.\n",
    "# →\n",
    "# ZYdZYac     # \"ab\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "# Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면\n",
    "# Y=ab        # 가장 많이 등장한 바이트 쌍 \"ZY\"를 \"X\"로 치환합니다.\n",
    "# →\n",
    "# XdXac\n",
    "# Z=aa\n",
    "# Y=ab\n",
    "# X=ZY       # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b3a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Step 1\n",
      "다음 문자 쌍을 치환: es\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n",
      "\n",
      ">> Step 2\n",
      "다음 문자 쌍을 치환: est\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 3\n",
      "다음 문자 쌍을 치환: lo\n",
      "변환된 Vocab:\n",
      " {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 4\n",
      "다음 문자 쌍을 치환: low\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 5\n",
      "다음 문자 쌍을 치환: ne\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n",
      "\n",
      "Merged Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다.\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    문자 쌍(pair)과 단어 리스트(v_in)를 입력받아\n",
    "    각각의 단어에서 등장하는 문자 쌍을 치환합니다.\n",
    "    (하나의 글자처럼 취급)\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "\n",
    "    token_vocab.append(merge_tok)\n",
    "\n",
    "print(\"Merged Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6899620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.items()는 [('l o w ', 5), ('l o w e r ', 2), ('n e w e s t ', 6), ('w i d e s t ', 3)]와 같은 리스트를 반환합니다.\n",
    "# for 루프가 돌면서 이 쌍들이 word와 freq 변수에 차례대로 할당(unpacking)됩니다.\n",
    "\n",
    "# 첫 번째 루프:\n",
    "\n",
    "# word = 'l o w '\n",
    "\n",
    "# freq = 5\n",
    "\n",
    "# 두 번째 루프:\n",
    "\n",
    "# word = 'l o w e r '\n",
    "\n",
    "# freq = 2\n",
    "\n",
    "# 요약: .items()는 딕셔너리에서 Key와 Value를 한 세트로 꺼내어 for 루프에서 사용하게 해주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding 레이어는 단어의 개수 x Embedding 차원 수 의 Weight를 생성하기 때문에 \n",
    "# # 단어의 개수가 줄어드는 것은 곧 메모리의 절약으로 이어집니다. \n",
    "# # 많은 데이터가 곧 정확도로 이어지기 때문에 이런 기여는 굉장히 의미가 있습니다!\n",
    "\n",
    "# 하지만 아직도! 완벽하다고는 할 수 없습니다. 만약 수많은 데이터를 사용해 만든 BPE 사전으로 모델을 학습시키고 문장을 생성하게 했다고 합시다. \n",
    "# 그게 [i, am, a, b, o, y, a, n, d, you, are, a, gir, l]이라면, 어떤 기준으로 이들을 결합해서 문장을 복원하죠? \n",
    "# 몽땅 한꺼번에 합쳤다간 끔찍한 일이 벌어질 것만 같습니다...\n",
    "\n",
    "\n",
    "Wordpiece Model(WPM)\n",
    "\n",
    "\n",
    "공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가합니다.\n",
    "빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합칩니다. (더 '그럴듯한' 토큰을 만들어냅니다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c647520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # 1. How BPE (Frequency) Chooses\n",
    "# # # # BPE just counts all adjacent pairs:\n",
    "\n",
    "# # # # ('e', 'a'): 10 (from _ t e a)\n",
    "\n",
    "# # # # ('r', 'e'): 10 (from _ r e a d) + 10 (from _ r e s t) = 20\n",
    "\n",
    "# # # # ('e', 'd'): 10 (from _ b e d)\n",
    "\n",
    "# # # # ('e', 's'): 10 (from _ r e s t)\n",
    "\n",
    "# # # # ('e', 't'): 10 (from _ w e t)\n",
    "\n",
    "# # # # ... (and all other pairs like _r, _t, etc.)\n",
    "\n",
    "# # # BPE's Choice: The pair ('r', 'e') has the highest absolute frequency (20). BPE would merge r and e to create a new token, re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 2. How WPM (Likelihood) Chooses\n",
    "# # WPM looks at the statistics differently. It calculates a score for each potential merge. A simplified way to think about this score is:\n",
    "\n",
    "# # Score(A, B) = count(A, B) / ( count(A) * count(B) )\n",
    "\n",
    "# # This score measures how often A and B appear together versus how often they appear independently. A higher score means the pair is \"tighter\" or more \"destined\" to be together.\n",
    "\n",
    "# # First, let's get the counts of the individual tokens:\n",
    "\n",
    "# # count('r'): 10 + 10 = 20\n",
    "\n",
    "# # count('e'): 10 + 10 + 10 + 10 + 10 = 50\n",
    "\n",
    "# # count('a'): 10\n",
    "\n",
    "# # count('d'): 10 + 10 = 20\n",
    "\n",
    "# # count('s'): 10\n",
    "\n",
    "# # count('t'): 10 + 10 = 20\n",
    "\n",
    "# # ... (and so on)\n",
    "\n",
    "# # Now, let's calculate the \"likelihood score\" for the most frequent pairs:\n",
    "\n",
    "# # Score for ('r', 'e') (BPE's choice):\n",
    "\n",
    "# # count(r, e) = 20\n",
    "\n",
    "# # count(r) = 20\n",
    "\n",
    "# # count(e) = 50\n",
    "\n",
    "# # Score = 20 / (20 * 50) = 20 / 1000 = 0.02\n",
    "\n",
    "# # Score for ('e', 'a'):\n",
    "\n",
    "# # count(e, a) = 10\n",
    "\n",
    "# # count(e) = 50\n",
    "\n",
    "# # count(a) = 10\n",
    "\n",
    "# # Score = 10 / (50 * 10) = 10 / 500 = 0.02\n",
    "\n",
    "# # Score for ('e', 's'):\n",
    "\n",
    "# # count(e, s) = 10\n",
    "\n",
    "# # count(e) = 50\n",
    "\n",
    "# # count(s) = 10\n",
    "\n",
    "# # Score = 10 / (50 * 10) = 10 / 500 = 0.02\n",
    "\n",
    "# # All of these pairs involving 'e' get a low score. Why? Because 'e' is a very common token that appears next to many different letters (r, a, d, s, t).\n",
    "# #  Merging 'e' with any single one of them (like 'r') isn't a \"special\" or \"high-likelihood\" event.\n",
    "\n",
    "# # WPM's Choice: WPM would likely find a different pair, one where the two tokens are more \"exclusive\" to each other. \n",
    "# # For example, in a larger corpus, a pair like ('q', 'u') would have a very high likelihood score, because 'q' almost exclusively appears next to 'u'. \n",
    "# # Even if ('q', 'u') only appeared 15 times (less than ('r', 'e')), its likelihood score would be much higher, and WPM would merge it first\n",
    "\n",
    "\n",
    "# \"How much more likely is the pair (A, B) to occur together than we would expect them to, given how often A and B appear independently?\"\n",
    "\n",
    "\n",
    "# Example 1: The \"Strong Bond\" Pair (q, u)\n",
    "# P(u) (By Chance): How often does 'u' appear in English? It's fairly common. Let's say P(u) = 2% (or 0.02).\n",
    "\n",
    "# P(u | q) (Given): Given that you just saw a 'q', what's the probability the next letter is 'u'? It's almost certain. Let's say P(u | q) = 99% (or 0.99).\n",
    "\n",
    "# Now, let's calculate the ratio: Score = P(u | q) / P(u) = 0.99 / 0.02 = 49.5\n",
    "\n",
    "# Interpretation: Seeing a 'u' after a 'q' is 49.5 times more likely than just seeing a 'u' by chance. This is a massive signal!\n",
    "# The bond between q and u is incredibly strong and not just a coincidence. WPM would merge this pair very early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp\n",
    "\n",
    "# 형태소 기반의 토크나이저가 미등록 단어에 취약하기 때문에 WordPiece Model을 사용하는 것처럼, 형태소 기반인 koNLPy의 단점을 해결하기 위해 soynlp를 사용할 수 있습니다.\n",
    "\n",
    "# soynlp의 토크나이저는 '학습데이터를 이용하지 않으면서 데이터에 존재하는 단어를 찾거나, 문장을 단어열로 분해, 혹은 품사 판별을 할 수 있는 비지도학습 접근법을 지향합니다' 라고 밝히고 있는데요.\n",
    "\n",
    "# 문장에서 처음 단어를 받아들일 때 단어의 경계를 알아야 올바르게 토큰화를 할 수 있습니다. 이때 단어의 경계를 비지도학습을 통해 결정하겠다는 말이에요. 비지도학습을 통한 방법이기 때문에 미등록 단어도 토큰화가 가능합니다. \n",
    "# 여기서 비지도학습을 가능케 하는 것이 통계적인 방법이라서 soynlp를 통계 기반 토크나이저로 분류하기도 합니다.\n",
    "\n",
    "# 트와이스가 한 단어임을 인지하기 위해서 트, 트와, 트와이, 트와이스 각각 다음 글자의 확률을 계산해서 비교한다고 생각하면 좋습니다. 수학적으로 자세한 내용은 여기서는 다루지 않습니다. 다만 koNLPy외에도 soynlp가 있다는 점을 기억해 주세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a546dd5",
   "metadata": {},
   "source": [
    "<!-- FastText\n",
    "Word2Vec은 정말 좋은 방법이지만, 연산의 빈부격차가 존재했습니다. 자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있습니다. FastText는 이를 해결하기 위해 BPE와 비슷한 아이디어를 적용했습니다. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af78ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68602e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e315328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
